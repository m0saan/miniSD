{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self Attention mechanism for sequence data.\n",
    "    \n",
    "    Attributes:\n",
    "        scale (float): Scaling factor for the attention scores.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        d_head (int): Dimension of each attention head.\n",
    "        QKV (nn.Linear): Linear layer for Query, Key, Value.\n",
    "        O (nn.Linear): Linear output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_embed, n_heads: int = 4, qkv_bias=True, out_bias=True) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention class.\n",
    "        \n",
    "        Args:\n",
    "            d_embed (int): Dimension of the embedding.\n",
    "            n_heads (int): Number of attention heads. Defaults to 4.\n",
    "            qkv_bias (bool): If True, adds bias to QKV linear layer. Defaults to True.\n",
    "            out_bias (bool): If True, adds bias to O linear layer. Defaults to True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "        self.scale = self.d_head ** -0.5\n",
    "        self.QKV = nn.Linear(d_embed, d_embed * 3, bias=qkv_bias)\n",
    "        self.O = nn.Linear(d_embed, d_embed, bias=out_bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the SelfAttention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_embed).\n",
    "            mask (bool): If True, applies the attention mask. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x: (batch_size, height*width, channels)\n",
    "        # x: (batch_size, seq_len, d_embed)\n",
    "        in_shape = x.shape\n",
    "        bs, seq_len, d_embed = x.shape\n",
    "        q, k, v = self.QKV(x).chunk(3, dim=-1) # (batch_size, seq_len, d_embed)@(d_embed, d_embed*3) -> (batch_size, seq_len, d_embed*3) -> (3x) (batch_size, seq_len, d_embed)\n",
    "        \n",
    "        # (batch_size, seq_len, d_embed) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        q = q.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if not mask:\n",
    "            mask = torch.ones_like(attn_scores).bool().triu(1) # (batch_size, n_heads, seq_len, seq_len)\n",
    "            attn_scores.masked_fill_(mask, -1e9)\n",
    "        weights = F.softmax(attn_scores, dim=-1)\n",
    "        output = weights @ v # (batch_size, n_heads, seq_len, seq_len) -> (batch_size, n_heads, seq_len, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        output = output.transpose(1, 2).contiguous().view(in_shape) # (batch_size, n_heads, seq_len, d_head) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, seq_len, d_embed)\n",
    "        return self.O(output) # (batch_size, seq_len, d_embed)@(d_embed, d_embed) -> (batch_size, seq_len, d_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int = 49408\n",
    "    hidden_size: int = 768\n",
    "    seq_len: int = 77\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    layer_norm_eps: float = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that converts input ids to embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(config.seq_len, config.hidden_size))\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        embeddings = self.token_embedding(input_ids) + self.position_embedding # (batch_size, seq_len, hidden_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.attention = SelfAttention(config.hidden_size, config.n_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        x = x + self.attention(self.layer_norm1(x), mask=True)\n",
    "        return x + self.mlp(self.layer_norm2(x)) # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of OpenAI's CLIP model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = CLIPEmbedding(config)\n",
    "        self.layers = nn.ModuleList([CLIPBlock(config) for _ in range(config.n_layers)])\n",
    "        # self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
    "        token_embeddings = self.embeddings(x.type(torch.long)) # (batch_size, seq_len) -> (batch_size, seq_len, hidden_size)\n",
    "        for layer in self.layers:\n",
    "            token_embeddings = layer(token_embeddings)\n",
    "        return token_embeddings # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a test code\n",
    "input_ids = torch.randint(0, 49408, (1, 77))\n",
    "config = Config()\n",
    "clip = CLIP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters(model):\n",
    "    for name, param in model.state_dict().items():\n",
    "        print(name)\n",
    "        \n",
    "# Assuming 'model' is your Pytorch model\n",
    "# print_parameters(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/checkpoints/v1-5-pruned-emaonly.ckpt'\n",
    "device = 'mps'\n",
    "original_model = torch.load(input_file, map_location=device, weights_only = False)[\"state_dict\"]\n",
    "converted = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted['clip'] = {}\n",
    "\n",
    "converted['clip']['embeddings.token_embedding.weight'] = original_model['cond_stage_model.transformer.text_model.embeddings.token_embedding.weight']\n",
    "converted['clip']['embeddings.position_embedding'] = original_model['cond_stage_model.transformer.text_model.embeddings.position_embedding.weight']\n",
    "converted['clip']['layers.0.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.0.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.0.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight']\n",
    "converted['clip']['layers.0.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias']\n",
    "converted['clip']['layers.0.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight']\n",
    "converted['clip']['layers.0.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias']\n",
    "converted['clip']['layers.0.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight']\n",
    "converted['clip']['layers.0.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias']\n",
    "converted['clip']['layers.0.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight']\n",
    "converted['clip']['layers.0.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias']\n",
    "converted['clip']['layers.1.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.1.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.1.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight']\n",
    "converted['clip']['layers.1.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias']\n",
    "converted['clip']['layers.1.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight']\n",
    "converted['clip']['layers.1.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias']\n",
    "converted['clip']['layers.1.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight']\n",
    "converted['clip']['layers.1.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias']\n",
    "converted['clip']['layers.1.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight']\n",
    "converted['clip']['layers.1.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias']\n",
    "converted['clip']['layers.2.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.2.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.2.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight']\n",
    "converted['clip']['layers.2.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias']\n",
    "converted['clip']['layers.2.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight']\n",
    "converted['clip']['layers.2.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias']\n",
    "converted['clip']['layers.2.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight']\n",
    "converted['clip']['layers.2.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias']\n",
    "converted['clip']['layers.2.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight']\n",
    "converted['clip']['layers.2.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias']\n",
    "converted['clip']['layers.3.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.3.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.3.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight']\n",
    "converted['clip']['layers.3.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias']\n",
    "converted['clip']['layers.3.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight']\n",
    "converted['clip']['layers.3.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias']\n",
    "converted['clip']['layers.3.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight']\n",
    "converted['clip']['layers.3.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias']\n",
    "converted['clip']['layers.3.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight']\n",
    "converted['clip']['layers.3.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias']\n",
    "converted['clip']['layers.4.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.4.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.4.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight']\n",
    "converted['clip']['layers.4.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias']\n",
    "converted['clip']['layers.4.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight']\n",
    "converted['clip']['layers.4.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias']\n",
    "converted['clip']['layers.4.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight']\n",
    "converted['clip']['layers.4.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias']\n",
    "converted['clip']['layers.4.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight']\n",
    "converted['clip']['layers.4.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias']\n",
    "converted['clip']['layers.5.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.5.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.5.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight']\n",
    "converted['clip']['layers.5.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias']\n",
    "converted['clip']['layers.5.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight']\n",
    "converted['clip']['layers.5.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias']\n",
    "converted['clip']['layers.5.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight']\n",
    "converted['clip']['layers.5.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias']\n",
    "converted['clip']['layers.5.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight']\n",
    "converted['clip']['layers.5.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias']\n",
    "converted['clip']['layers.6.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.6.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.6.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight']\n",
    "converted['clip']['layers.6.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias']\n",
    "converted['clip']['layers.6.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight']\n",
    "converted['clip']['layers.6.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias']\n",
    "converted['clip']['layers.6.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight']\n",
    "converted['clip']['layers.6.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias']\n",
    "converted['clip']['layers.6.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight']\n",
    "converted['clip']['layers.6.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias']\n",
    "converted['clip']['layers.7.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.7.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.7.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight']\n",
    "converted['clip']['layers.7.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias']\n",
    "converted['clip']['layers.7.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight']\n",
    "converted['clip']['layers.7.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias']\n",
    "converted['clip']['layers.7.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight']\n",
    "converted['clip']['layers.7.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias']\n",
    "converted['clip']['layers.7.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight']\n",
    "converted['clip']['layers.7.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias']\n",
    "converted['clip']['layers.8.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.8.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.8.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight']\n",
    "converted['clip']['layers.8.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias']\n",
    "converted['clip']['layers.8.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight']\n",
    "converted['clip']['layers.8.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias']\n",
    "converted['clip']['layers.8.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight']\n",
    "converted['clip']['layers.8.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias']\n",
    "converted['clip']['layers.8.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight']\n",
    "converted['clip']['layers.8.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias']\n",
    "converted['clip']['layers.9.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.9.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.9.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight']\n",
    "converted['clip']['layers.9.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias']\n",
    "converted['clip']['layers.9.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight']\n",
    "converted['clip']['layers.9.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias']\n",
    "converted['clip']['layers.9.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight']\n",
    "converted['clip']['layers.9.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias']\n",
    "converted['clip']['layers.9.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight']\n",
    "converted['clip']['layers.9.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias']\n",
    "converted['clip']['layers.10.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.10.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.10.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight']\n",
    "converted['clip']['layers.10.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias']\n",
    "converted['clip']['layers.10.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight']\n",
    "converted['clip']['layers.10.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias']\n",
    "converted['clip']['layers.10.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight']\n",
    "converted['clip']['layers.10.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias']\n",
    "converted['clip']['layers.10.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight']\n",
    "converted['clip']['layers.10.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias']\n",
    "converted['clip']['layers.11.attention.O.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.11.attention.O.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.11.layer_norm1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight']\n",
    "converted['clip']['layers.11.layer_norm1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias']\n",
    "converted['clip']['layers.11.mlp.0.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight']\n",
    "converted['clip']['layers.11.mlp.0.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias']\n",
    "converted['clip']['layers.11.mlp.2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight']\n",
    "converted['clip']['layers.11.mlp.2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias']\n",
    "converted['clip']['layers.11.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight']\n",
    "converted['clip']['layers.11.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias']\n",
    "converted['clip']['layers.11.layer_norm2.weight'] = original_model['cond_stage_model.transformer.text_model.final_layer_norm.weight']\n",
    "converted['clip']['layers.11.layer_norm2.bias'] = original_model['cond_stage_model.transformer.text_model.final_layer_norm.bias']\n",
    "converted['clip']['layers.0.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.0.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.1.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.1.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.2.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.2.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.3.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.3.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.4.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.4.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.5.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.5.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.6.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.6.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.7.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.7.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.8.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.8.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.9.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.9.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.10.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.10.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias']), 0)\n",
    "converted['clip']['layers.11.attention.QKV.weight'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight'], original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight']), 0)\n",
    "converted['clip']['layers.11.attention.QKV.bias'] = torch.cat((original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias'], original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias']), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "clip.load_state_dict(converted['clip'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted['clip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

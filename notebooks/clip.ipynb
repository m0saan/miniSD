{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_embed, n_heads: int = 4, qkv_bias=True, out_bias=True) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.scale = d_embed ** -0.5\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "        self.QKV = nn.Linear(d_embed, d_embed * 3, bias=qkv_bias)\n",
    "        self.O = nn.Linear(d_embed, d_embed, bias=out_bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: bool = False) -> torch.Tensor:\n",
    "        # x: (batch_size, height*width, channels)\n",
    "        # x: (batch_size, seq_len, d_embed)\n",
    "        in_shape = x.shape\n",
    "        bs, seq_len, d_embed = x.shape\n",
    "        q, k, v = self.QKV(x).chunk(3, dim=-1) # (batch_size, seq_len, d_embed)@(d_embed, d_embed*3) -> (batch_size, seq_len, d_embed*3) -> (3x) (batch_size, seq_len, d_embed)\n",
    "        \n",
    "        # (batch_size, seq_len, d_embed) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        q = q.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if mask is not False:\n",
    "            mask = torch.ones_like(attn_scores).bool().triu(1) # (batch_size, n_heads, seq_len, seq_len)\n",
    "            attn_scores.masked_fill_(mask, -1e9)\n",
    "        weights = F.softmax(attn_scores, dim=-1)\n",
    "        output = weights @ v # (batch_size, n_heads, seq_len, seq_len) -> (batch_size, n_heads, seq_len, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        output = output.transpose(1, 2).contiguous().view(in_shape) # (batch_size, n_heads, seq_len, d_head) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, seq_len, d_embed)\n",
    "        return self.O(output) # (batch_size, seq_len, d_embed)@(d_embed, d_embed) -> (batch_size, seq_len, d_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int = 49408\n",
    "    hidden_size: int = 768\n",
    "    seq_len: int = 77\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    layer_norm_eps: float = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that converts input ids to embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(config.seq_len, config.hidden_size))\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        embeddings = self.token_embedding(input_ids) + self.position_embedding # (batch_size, seq_len, hidden_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.attention = SelfAttention(config.hidden_size, config.n_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        x = x + self.attention(self.layer_norm1(x), mask=True)\n",
    "        return x + self.mlp(self.layer_norm2(x)) # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of OpenAI's CLIP model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = CLIPEmbedding(config)\n",
    "        self.layers = nn.ModuleList([CLIPBlock(config) for _ in range(config.n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
    "        token_embeddings = self.embeddings(x.type(torch.long)) # (batch_size, seq_len) -> (batch_size, seq_len, hidden_size)\n",
    "        for layer in self.layers:\n",
    "            token_embeddings = layer(token_embeddings)\n",
    "        return self.layer_norm(token_embeddings) # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (embeddings): CLIPEmbedding(\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x CLIPBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): SelfAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a test code\n",
    "input_ids = torch.randint(0, 49408, (1, 77))\n",
    "config = Config()\n",
    "clip = CLIP(config)\n",
    "clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

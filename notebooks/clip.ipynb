{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self Attention mechanism for sequence data.\n",
    "    \n",
    "    Attributes:\n",
    "        scale (float): Scaling factor for the attention scores.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        d_head (int): Dimension of each attention head.\n",
    "        QKV (nn.Linear): Linear layer for Query, Key, Value.\n",
    "        O (nn.Linear): Linear output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_embed, n_heads: int = 4, qkv_bias=True, out_bias=True) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention class.\n",
    "        \n",
    "        Args:\n",
    "            d_embed (int): Dimension of the embedding.\n",
    "            n_heads (int): Number of attention heads. Defaults to 4.\n",
    "            qkv_bias (bool): If True, adds bias to QKV linear layer. Defaults to True.\n",
    "            out_bias (bool): If True, adds bias to O linear layer. Defaults to True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "        self.scale = self.d_head ** -0.5\n",
    "        self.QKV = nn.Linear(d_embed, d_embed * 3, bias=qkv_bias)\n",
    "        self.O = nn.Linear(d_embed, d_embed, bias=out_bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the SelfAttention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_embed).\n",
    "            mask (bool): If True, applies the attention mask. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x: (batch_size, height*width, channels)\n",
    "        # x: (batch_size, seq_len, d_embed)\n",
    "        in_shape = x.shape\n",
    "        bs, seq_len, d_embed = x.shape\n",
    "        q, k, v = self.QKV(x).chunk(3, dim=-1) # (batch_size, seq_len, d_embed)@(d_embed, d_embed*3) -> (batch_size, seq_len, d_embed*3) -> (3x) (batch_size, seq_len, d_embed)\n",
    "        \n",
    "        # (batch_size, seq_len, d_embed) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        q = q.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if not mask:\n",
    "            mask = torch.ones_like(attn_scores).bool().triu(1) # (batch_size, n_heads, seq_len, seq_len)\n",
    "            attn_scores.masked_fill_(mask, -1e9)\n",
    "        weights = F.softmax(attn_scores, dim=-1)\n",
    "        output = weights @ v # (batch_size, n_heads, seq_len, seq_len) -> (batch_size, n_heads, seq_len, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        output = output.transpose(1, 2).contiguous().view(in_shape) # (batch_size, n_heads, seq_len, d_head) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, seq_len, d_embed)\n",
    "        return self.O(output) # (batch_size, seq_len, d_embed)@(d_embed, d_embed) -> (batch_size, seq_len, d_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int = 49408\n",
    "    hidden_size: int = 768\n",
    "    seq_len: int = 77\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    layer_norm_eps: float = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that converts input ids to embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(config.seq_len, config.hidden_size))\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        embeddings = self.token_embedding(input_ids) + self.position_embedding # (batch_size, seq_len, hidden_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.attention = SelfAttention(config.hidden_size, config.n_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        x = x + self.attention(self.layer_norm1(x), mask=True)\n",
    "        return x + self.mlp(self.layer_norm2(x)) # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of OpenAI's CLIP model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = CLIPEmbedding(config)\n",
    "        self.layers = nn.ModuleList([CLIPBlock(config) for _ in range(config.n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
    "        token_embeddings = self.embeddings(x.type(torch.long)) # (batch_size, seq_len) -> (batch_size, seq_len, hidden_size)\n",
    "        for layer in self.layers:\n",
    "            token_embeddings = layer(token_embeddings)\n",
    "        return self.layer_norm(token_embeddings) # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',\n",
       "  CLIP(\n",
       "    (embeddings): CLIPEmbedding(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x CLIPBlock(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )),\n",
       " ('embeddings',\n",
       "  CLIPEmbedding(\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "  )),\n",
       " ('embeddings.token_embedding', Embedding(49408, 768)),\n",
       " ('layers',\n",
       "  ModuleList(\n",
       "    (0-11): 12 x CLIPBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): SelfAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('layers.0',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.0.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.0.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.0.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.0.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.0.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.0.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.0.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.0.mlp.1', GELU(approximate='none')),\n",
       " ('layers.0.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.1',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.1.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.1.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.1.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.1.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.1.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.1.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.1.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.1.mlp.1', GELU(approximate='none')),\n",
       " ('layers.1.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.2',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.2.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.2.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.2.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.2.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.2.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.2.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.2.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.2.mlp.1', GELU(approximate='none')),\n",
       " ('layers.2.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.3',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.3.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.3.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.3.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.3.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.3.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.3.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.3.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.3.mlp.1', GELU(approximate='none')),\n",
       " ('layers.3.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.4',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.4.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.4.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.4.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.4.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.4.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.4.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.4.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.4.mlp.1', GELU(approximate='none')),\n",
       " ('layers.4.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.5',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.5.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.5.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.5.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.5.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.5.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.5.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.5.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.5.mlp.1', GELU(approximate='none')),\n",
       " ('layers.5.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.6',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.6.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.6.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.6.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.6.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.6.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.6.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.6.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.6.mlp.1', GELU(approximate='none')),\n",
       " ('layers.6.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.7',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.7.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.7.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.7.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.7.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.7.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.7.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.7.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.7.mlp.1', GELU(approximate='none')),\n",
       " ('layers.7.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.8',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.8.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.8.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.8.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.8.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.8.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.8.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.8.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.8.mlp.1', GELU(approximate='none')),\n",
       " ('layers.8.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.9',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.9.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.9.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.9.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.9.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.9.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.9.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.9.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.9.mlp.1', GELU(approximate='none')),\n",
       " ('layers.9.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.10',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.10.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.10.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.10.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.10.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.10.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.10.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.10.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.10.mlp.1', GELU(approximate='none')),\n",
       " ('layers.10.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layers.11',\n",
       "  CLIPBlock(\n",
       "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SelfAttention(\n",
       "      (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers.11.layer_norm1',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.11.layer_norm2',\n",
       "  LayerNorm((768,), eps=1e-05, elementwise_affine=True)),\n",
       " ('layers.11.attention',\n",
       "  SelfAttention(\n",
       "    (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.11.attention.QKV',\n",
       "  Linear(in_features=768, out_features=2304, bias=True)),\n",
       " ('layers.11.attention.O',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('layers.11.mlp',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )),\n",
       " ('layers.11.mlp.0', Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('layers.11.mlp.1', GELU(approximate='none')),\n",
       " ('layers.11.mlp.2', Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('layer_norm', LayerNorm((768,), eps=1e-05, elementwise_affine=True))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a test code\n",
    "input_ids = torch.randint(0, 49408, (1, 77))\n",
    "config = Config()\n",
    "clip = CLIP(config)\n",
    "list(clip.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clip.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclip.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m original_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(input_file, map_location\u001b[39m=\u001b[39;49mdevice, weights_only \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m converted \u001b[39m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m converted[\u001b[39m'\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/mambaforge/envs/fastai/lib/python3.11/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/mambaforge/envs/fastai/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/mambaforge/envs/fastai/lib/python3.11/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clip.pth'"
     ]
    }
   ],
   "source": [
    "input_file = 'clip.pth'\n",
    "device = 'mps'\n",
    "original_model = torch.load(input_file, map_location=device, weights_only = False)[\"state_dict\"]\n",
    "converted = {}\n",
    "converted['clip'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted['clip']['embedding.token_embedding.weight'] = original_model['cond_stage_model.transformer.text_model.embeddings.token_embedding.weight']\n",
    "converted['clip']['embedding.position_embedding'] = original_model['cond_stage_model.transformer.text_model.embeddings.position_embedding.weight']\n",
    "converted['clip']['layers.0.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.0.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.0.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight']\n",
    "converted['clip']['layers.0.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias']\n",
    "converted['clip']['layers.0.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight']\n",
    "converted['clip']['layers.0.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias']\n",
    "converted['clip']['layers.0.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight']\n",
    "converted['clip']['layers.0.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias']\n",
    "converted['clip']['layers.0.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight']\n",
    "converted['clip']['layers.0.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias']\n",
    "converted['clip']['layers.1.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.1.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.1.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight']\n",
    "converted['clip']['layers.1.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias']\n",
    "converted['clip']['layers.1.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight']\n",
    "converted['clip']['layers.1.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias']\n",
    "converted['clip']['layers.1.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight']\n",
    "converted['clip']['layers.1.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias']\n",
    "converted['clip']['layers.1.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight']\n",
    "converted['clip']['layers.1.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias']\n",
    "converted['clip']['layers.2.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.2.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.2.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight']\n",
    "converted['clip']['layers.2.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias']\n",
    "converted['clip']['layers.2.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight']\n",
    "converted['clip']['layers.2.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias']\n",
    "converted['clip']['layers.2.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight']\n",
    "converted['clip']['layers.2.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias']\n",
    "converted['clip']['layers.2.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight']\n",
    "converted['clip']['layers.2.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias']\n",
    "converted['clip']['layers.3.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.3.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.3.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight']\n",
    "converted['clip']['layers.3.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias']\n",
    "converted['clip']['layers.3.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight']\n",
    "converted['clip']['layers.3.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias']\n",
    "converted['clip']['layers.3.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight']\n",
    "converted['clip']['layers.3.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias']\n",
    "converted['clip']['layers.3.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight']\n",
    "converted['clip']['layers.3.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias']\n",
    "converted['clip']['layers.4.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.4.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.4.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight']\n",
    "converted['clip']['layers.4.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias']\n",
    "converted['clip']['layers.4.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight']\n",
    "converted['clip']['layers.4.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias']\n",
    "converted['clip']['layers.4.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight']\n",
    "converted['clip']['layers.4.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias']\n",
    "converted['clip']['layers.4.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight']\n",
    "converted['clip']['layers.4.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias']\n",
    "converted['clip']['layers.5.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.5.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.5.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight']\n",
    "converted['clip']['layers.5.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias']\n",
    "converted['clip']['layers.5.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight']\n",
    "converted['clip']['layers.5.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias']\n",
    "converted['clip']['layers.5.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight']\n",
    "converted['clip']['layers.5.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias']\n",
    "converted['clip']['layers.5.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight']\n",
    "converted['clip']['layers.5.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias']\n",
    "converted['clip']['layers.6.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.6.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.6.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight']\n",
    "converted['clip']['layers.6.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias']\n",
    "converted['clip']['layers.6.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight']\n",
    "converted['clip']['layers.6.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias']\n",
    "converted['clip']['layers.6.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight']\n",
    "converted['clip']['layers.6.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias']\n",
    "converted['clip']['layers.6.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight']\n",
    "converted['clip']['layers.6.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias']\n",
    "converted['clip']['layers.7.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.7.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.7.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight']\n",
    "converted['clip']['layers.7.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias']\n",
    "converted['clip']['layers.7.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight']\n",
    "converted['clip']['layers.7.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias']\n",
    "converted['clip']['layers.7.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight']\n",
    "converted['clip']['layers.7.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias']\n",
    "converted['clip']['layers.7.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight']\n",
    "converted['clip']['layers.7.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias']\n",
    "converted['clip']['layers.8.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.8.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.8.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight']\n",
    "converted['clip']['layers.8.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias']\n",
    "converted['clip']['layers.8.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight']\n",
    "converted['clip']['layers.8.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias']\n",
    "converted['clip']['layers.8.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight']\n",
    "converted['clip']['layers.8.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias']\n",
    "converted['clip']['layers.8.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight']\n",
    "converted['clip']['layers.8.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias']\n",
    "converted['clip']['layers.9.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.9.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.9.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight']\n",
    "converted['clip']['layers.9.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias']\n",
    "converted['clip']['layers.9.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight']\n",
    "converted['clip']['layers.9.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias']\n",
    "converted['clip']['layers.9.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight']\n",
    "converted['clip']['layers.9.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias']\n",
    "converted['clip']['layers.9.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight']\n",
    "converted['clip']['layers.9.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias']\n",
    "converted['clip']['layers.10.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.10.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.10.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight']\n",
    "converted['clip']['layers.10.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias']\n",
    "converted['clip']['layers.10.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight']\n",
    "converted['clip']['layers.10.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias']\n",
    "converted['clip']['layers.10.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight']\n",
    "converted['clip']['layers.10.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias']\n",
    "converted['clip']['layers.10.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight']\n",
    "converted['clip']['layers.10.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias']\n",
    "converted['clip']['layers.11.attention.out_proj.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight']\n",
    "converted['clip']['layers.11.attention.out_proj.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias']\n",
    "converted['clip']['layers.11.layernorm_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight']\n",
    "converted['clip']['layers.11.layernorm_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias']\n",
    "converted['clip']['layers.11.linear_1.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight']\n",
    "converted['clip']['layers.11.linear_1.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias']\n",
    "converted['clip']['layers.11.linear_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight']\n",
    "converted['clip']['layers.11.linear_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias']\n",
    "converted['clip']['layers.11.layernorm_2.weight'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight']\n",
    "converted['clip']['layers.11.layernorm_2.bias'] = original_model['cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias']\n",
    "converted['clip']['layernorm.weight'] = original_model['cond_stage_model.transformer.text_model.final_layer_norm.weight']\n",
    "converted['clip']['layernorm.bias'] = original_model['cond_stage_model.transformer.text_model.final_layer_norm.bias']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision.transforms import CenterCrop\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    time_embedding_dim: int = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self Attention mechanism for sequence data.\n",
    "    \n",
    "    Attributes:\n",
    "        scale (float): Scaling factor for the attention scores.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        d_head (int): Dimension of each attention head.\n",
    "        QKV (nn.Linear): Linear layer for Query, Key, Value.\n",
    "        O (nn.Linear): Linear output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_embed, n_heads: int = 4, qkv_bias=True, out_bias=True) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention class.\n",
    "        \n",
    "        Args:\n",
    "            d_embed (int): Dimension of the embedding.\n",
    "            n_heads (int): Number of attention heads. Defaults to 4.\n",
    "            qkv_bias (bool): If True, adds bias to QKV linear layer. Defaults to True.\n",
    "            out_bias (bool): If True, adds bias to O linear layer. Defaults to True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "        self.scale = self.d_head ** -0.5\n",
    "        self.QKV = nn.Linear(d_embed, d_embed * 3, bias=qkv_bias)\n",
    "        self.O = nn.Linear(d_embed, d_embed, bias=out_bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the SelfAttention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_embed).\n",
    "            mask (bool): If True, applies the attention mask. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x: (batch_size, height*width, channels)\n",
    "        # x: (batch_size, seq_len, d_embed)\n",
    "        in_shape = x.shape\n",
    "        bs, seq_len, d_embed = x.shape\n",
    "        q, k, v = self.QKV(x).chunk(3, dim=-1) # (batch_size, seq_len, d_embed)@(d_embed, d_embed*3) -> (batch_size, seq_len, d_embed*3) -> (3x) (batch_size, seq_len, d_embed)\n",
    "        \n",
    "        # (batch_size, seq_len, d_embed) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        q = q.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(bs, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if not mask:\n",
    "            mask = torch.ones_like(attn_scores).bool().triu(1) # (batch_size, n_heads, seq_len, seq_len)\n",
    "            attn_scores.masked_fill_(mask, -1e9)\n",
    "        weights = F.softmax(attn_scores, dim=-1)\n",
    "        output = weights @ v # (batch_size, n_heads, seq_len, seq_len) -> (batch_size, n_heads, seq_len, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        output = output.transpose(1, 2).contiguous().view(in_shape) # (batch_size, n_heads, seq_len, d_head) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, seq_len, d_embed)\n",
    "        return self.O(output) # (batch_size, seq_len, d_embed)@(d_embed, d_embed) -> (batch_size, seq_len, d_embed)\n",
    "    \n",
    "    \n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross Attention mechanism for sequence data.\n",
    "    \n",
    "    Attributes:\n",
    "        scale (float): Scaling factor for the attention scores.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        d_head (int): Dimension of each attention head.\n",
    "        QKV (nn.Linear): Linear layer for Query, Key, Value.\n",
    "        O (nn.Linear): Linear output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_heads: int, d_embed: int, d_cross: int, qkv_bias=True, out_bias=True) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention class.\n",
    "        \n",
    "        Args:\n",
    "            d_embed (int): Dimension of the embedding.\n",
    "            n_heads (int): Number of attention heads. Defaults to 4.\n",
    "            qkv_bias (bool): If True, adds bias to QKV linear layer. Defaults to True.\n",
    "            out_bias (bool): If True, adds bias to O linear layer. Defaults to True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "        self.scale = self.d_head ** -0.5\n",
    "        self.Q = nn.Linear(d_embed, d_embed, bias=qkv_bias)\n",
    "        self.K = nn.Linear(d_cross, d_embed, bias=qkv_bias)\n",
    "        self.V = nn.Linear(d_cross, d_embed, bias=qkv_bias)\n",
    "        self.O = nn.Linear(d_embed, d_embed, bias=out_bias)\n",
    "        \n",
    "    def forward(self, x_q: torch.Tensor, x_kv: torch.TensorType) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the CrossAttention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_embed).\n",
    "            mask (bool): If True, applies the attention mask. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x_q(latents): (batch_size, seq_len_Q, d_embed_Q)\n",
    "        # x_kv(context): (batch_size, seq_len_KV, d_embed_KV)\n",
    "        in_shape = x_q.shape\n",
    "        bs, seq_len, d_embed = x_q.shape\n",
    "        \n",
    "        # (batch_size, seq_len, d_embed) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        q = self.Q(x_q).view(bs, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = self.K(x_kv).view(bs, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = self.V(x_kv).view(bs, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        weights = F.softmax(attn_scores, dim=-1)\n",
    "        output = weights @ v # (batch_size, n_heads, seq_len, seq_len) -> (batch_size, n_heads, seq_len, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
    "        output = output.transpose(1, 2).contiguous().view(in_shape) # (batch_size, n_heads, seq_len, d_head) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, seq_len, d_embed)\n",
    "        return self.O(output) # (batch_size, seq_len, d_embed)@(d_embed, d_embed) -> (batch_size, seq_len, d_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of the U-Net architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (downs): ModuleList()\n",
       "  (ups): ModuleList()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = Unet()\n",
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 388, 388])\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3) # in_chxout_ch\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3) # out_chxout_ch\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "# enc_block1 = Block(in_channels=1, out_channels=64)\n",
    "# x =  torch.randn(1, 1, 64, 64)\n",
    "# print(enc_block1(x).shape)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels: tuple = (3, 64, 128, 256, 512, 1024)) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_blocks = nn.ModuleList(Block(in_ch, out_ch) for in_ch, out_ch in zip(channels, channels[1:]))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return features\n",
    "        \n",
    "        \n",
    "encoder = Encoder().to('mps')\n",
    "x = torch.randn(1, 3, 572, 572).to('mps')\n",
    "ftrs = encoder(x)\n",
    "# for ftr in ftrs: print(ftr.shape)\n",
    "\n",
    "# an upsampling of the feature map\n",
    "# a 2x2 convolution (“up-convolution”) that halves the number of feature channels, \n",
    "# a concatenation with the correspondingly cropped feature map from the contracting path\n",
    "# two 3x3 convolutions, each followed by a ReLU\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.up_convs = nn.ModuleList(nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2) \n",
    "                                      for in_ch, out_ch in zip(channels, channels[1:]))\n",
    "        self.dec_blocks = nn.ModuleList(Block(in_ch, out_ch) for in_ch, out_ch in zip(channels, channels[1:]))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, features: list) -> torch.Tensor:\n",
    "        for up_conv, dec_block, feature in zip(self.up_convs, self.dec_blocks, features[::-1][1:]):\n",
    "            x = up_conv(x)\n",
    "            enc_feature = CenterCrop(x.shape[2:])(feature)\n",
    "            x = torch.cat([x, enc_feature], dim=1)\n",
    "            x = dec_block(x)\n",
    "        return x\n",
    "    \n",
    "decoder = Decoder().to('mps')\n",
    "x = torch.randn(1, 1024, 28, 28).to('mps')\n",
    "print(decoder(x, ftrs).shape)\n",
    "\n",
    "# >> (torch.Size([1, 64, 388, 388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz      = out_sz\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of the time embeddings used in Diffusion model.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_embedding_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(in_features=time_embedding_dim, out_features=time_embedding_dim*4) # 320x1280\n",
    "        self.linear_2 = nn.Linear(in_features=time_embedding_dim*4, out_features=time_embedding_dim*4) # 1280x1280\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_2(F.silu(self.linear_1(x))) # (1, 320) -> (1, 1280)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of the U-Net head.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.group_norm = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, in_channels, height, width) Bx320x64x64\n",
    "        x = F.silu(self.group_norm(x))\n",
    "        return self.conv(x) # Bx320x64x64 -> Bx4x64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x:  BxCxHxW -> BxCx2*Hx2*W\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_heads: int, n_embed: int, hidden_size: int = 768) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        channel_embed = n_heads * n_embed\n",
    "        self.group_norm = nn.GroupNorm(num_groups=32, num_channels=channel_embed)\n",
    "        self.conv_in = nn.Conv2d(in_channels=channel_embed, out_channels=channel_embed, kernel_size=1)\n",
    "        self.conv_out = nn.Conv2d(in_channels=channel_embed, out_channels=channel_embed, kernel_size=1)\n",
    "        self.layer_norm_1 = nn.LayerNorm(channel_embed)\n",
    "        self.layer_norm_2 = nn.LayerNorm(channel_embed)\n",
    "        self.layer_norm_3 = nn.LayerNorm(channel_embed)\n",
    "        self.attention_1 = SelfAttention(n_heads=n_heads, d_embed=channel_embed, qkv_bias=False)\n",
    "        self.attention_2 = CrossAttention(n_heads=n_heads, d_embed=channel_embed, d_cross=hidden_size, qkv_bias=False) \n",
    "        self.linear_1 = nn.Linear(in_features=channel_embed, out_features=channel_embed*4*2)\n",
    "        self.linear_2 = nn.Linear(in_features=channel_embed*4, out_features=channel_embed)\n",
    "        \n",
    "    def ffn(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residue = x\n",
    "        x, gate = self.linear_1(x).chunk(2, dim=-1)\n",
    "        x = x * F.gelu(gate)\n",
    "        return residue + self.linear_2(x)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, n_heads*n_embed, height, width)\n",
    "        # context: (batch_size, seq_len, hidden_size)\n",
    "        n, c, h, w = x.shape\n",
    "        long_residue = x\n",
    "        x = self.group_norm(x)\n",
    "        x = self.conv_in(x)\n",
    "        \n",
    "        x = x.view((n, c, h*w)).transpose(-1, -2) # (batch_size, c, height*width) -> (batch_size, height*width, c)\n",
    "        \n",
    "        # Layer Normalization + Self-Attention + Residual\n",
    "        x = x + self.attention_1(self.layer_norm_1(x)) # (batch_size, height*width, c)\n",
    "        \n",
    "        # Layer Normalization + Cross-Attention + Residual\n",
    "        x = x + self.attention_2(self.layer_norm_2(x), context) # (batch_size, height*width, c)\n",
    "        \n",
    "        # Layer Normalization + Feed Forward + Residual\n",
    "        x = self.ffn(self.layer_norm_3(x)) # (batch_size, height*width, c)\n",
    "        \n",
    "        x = x.transpose(-1, -2).view((n, c, h, w)) # (batch_size, height*width, c) -> (batch_size, c, height, width)\n",
    "        \n",
    "        return self.conv_out(x) + long_residue # (batch_size, c, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_embed_dim: int = 1280) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.group_norm = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        self.linear = nn.Linear(in_features=time_embed_dim, out_features=out_channels)\n",
    "        \n",
    "        self.group_norm_merged = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n",
    "        self.conv_merged = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            self.skip = nn.Identity()\n",
    "        else:\n",
    "            self.skip = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, in_channels, height, width) -> (batch_size, out_channels, height, width)\n",
    "        residue = x\n",
    "        x = self.conv(F.silu(self.group_norm(x)))\n",
    "        t = self.linear(F.silu(time)).unsqueeze(-1).unsqueeze(-1)\n",
    "        merged = x + t\n",
    "        return self.conv_merged(F.silu(self.group_norm_merged(merged))) + self.skip(residue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyLayer(nn.Sequential):\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        print(f'x: {x.shape}, time: {time.shape}, context: {context.shape}')\n",
    "        for layer in self:\n",
    "            if isinstance(layer, UnetAttentionBlock):\n",
    "                x = layer(x, context)\n",
    "            elif isinstance(layer, UnetResidualBlock):\n",
    "                x = layer(x, time)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder_layers():\n",
    "    return nn.ModuleList([\n",
    "        ApplyLayer(nn.Conv2d(in_channels=4, out_channels=320, kernel_size=3, padding=1)), # Bx4x64x64 -> Bx320x64x64\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=320, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx320x64x64 -> Bx320x64x64\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=320, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx320x64x64 -> Bx320x64x64\n",
    "        \n",
    "        ApplyLayer(nn.Conv2d(in_channels=320, out_channels=320, kernel_size=3, padding=1, stride=2)), # Bx320x64x64 -> Bx320x32x32\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=320, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx320x32x32 -> Bx320x32x32\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx320x32x32 -> Bx320x32x32\n",
    "        \n",
    "        ApplyLayer(nn.Conv2d(in_channels=640, out_channels=640, kernel_size=3, padding=1, stride=2)), # Bx640x32x32 -> Bx640x16x16\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx640x16x16 -> Bx640x16x16\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx640x16x16 -> Bx640x16x16\n",
    "        \n",
    "        ApplyLayer(nn.Conv2d(in_channels=1280, out_channels=1280, kernel_size=3, padding=1, stride=2)), # Bx1280x16x16 -> Bx1280x8x8\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=1280)),\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=1280)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder_layers():\n",
    "    return nn.ModuleList([\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280)), # Bx2560x8x8 -> Bx1280x8x8\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280)), # Bx2560x8x8 -> Bx1280x8x8\n",
    "        \n",
    "        ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280), Upsample(in_channels=1280)), # Bx2560x8x8 -> Bx1280x16x16\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx2560x16x16 -> Bx1280x16x16\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx2560x16x16 -> Bx1280x16x16\n",
    "        \n",
    "        ApplyLayer(UnetResidualBlock(in_channels=1920, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160), Upsample(in_channels=1280)), # Bx2560x16x16 -> Bx1280x32x32\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=1920, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx1920x32x32 -> Bx640x32x32\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx1280x32x32 -> Bx640x32x32\n",
    "        \n",
    "        ApplyLayer(UnetResidualBlock(in_channels=960, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80), Upsample(in_channels=640)), # Bx960x32x32 -> Bx640x64x64\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=960, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx960x32x32 -> Bx320x64x64\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx640x32x32 -> Bx320x64x64\n",
    "        ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx640x32x32 -> Bx320x64x64  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = self._make_encoder_layers()\n",
    "        self.bottle_neck = ApplyLayer(\n",
    "            UnetResidualBlock(in_channels=1280, out_channels=1280),\n",
    "            UnetAttentionBlock(n_heads=8, n_embed=160),\n",
    "            UnetResidualBlock(in_channels=1280, out_channels=1280)\n",
    "        )\n",
    "        self.decoder   = self._make_decoder_layers()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        # x: Bx4x64x64\n",
    "        # time: 1x1280\n",
    "        # context: Bx77x768\n",
    "        skip_connections = []\n",
    "        for enc_layer in self.encoder:\n",
    "            x = enc_layer(x, time, context)\n",
    "            skip_connections.append(x)\n",
    "            \n",
    "        x = self.bottle_neck(skip_connections[-1], time, context)\n",
    "        \n",
    "        for dec_layer in self.decoder:\n",
    "            x = torch.cat([x, skip_connections.pop()], dim=1)\n",
    "            x = dec_layer(x, time, context)\n",
    "        \n",
    "        return x # Bx320x64x64\n",
    "    \n",
    "    \n",
    "    def _make_encoder_layers(self):\n",
    "        return nn.ModuleList([\n",
    "            ApplyLayer(nn.Conv2d(in_channels=4, out_channels=320, kernel_size=3, padding=1)), # Bx4x64x64 -> Bx320x64x64\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=320, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx320x64x64 -> Bx320x64x64\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=320, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx320x64x64 -> Bx320x64x64\n",
    "            \n",
    "            ApplyLayer(nn.Conv2d(in_channels=320, out_channels=320, kernel_size=3, padding=1, stride=2)), # Bx320x64x64 -> Bx320x32x32\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=320, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx320x32x32 -> Bx320x32x32\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx320x32x32 -> Bx320x32x32\n",
    "            \n",
    "            ApplyLayer(nn.Conv2d(in_channels=640, out_channels=640, kernel_size=3, padding=1, stride=2)), # Bx640x32x32 -> Bx640x16x16\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx640x16x16 -> Bx640x16x16\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx640x16x16 -> Bx640x16x16\n",
    "            \n",
    "            ApplyLayer(nn.Conv2d(in_channels=1280, out_channels=1280, kernel_size=3, padding=1, stride=2)), # Bx1280x16x16 -> Bx1280x8x8\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=1280)),\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=1280)),\n",
    "        ])\n",
    "        \n",
    "    def _make_decoder_layers(self):\n",
    "        return nn.ModuleList([\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280)), # Bx2560x8x8 -> Bx1280x8x8\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280)), # Bx2560x8x8 -> Bx1280x8x8\n",
    "            \n",
    "            ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280), Upsample(in_channels=1280)), # Bx2560x8x8 -> Bx1280x16x16\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx2560x16x16 -> Bx1280x16x16\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=2560, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160)), # Bx2560x16x16 -> Bx1280x16x16\n",
    "            \n",
    "            ApplyLayer(UnetResidualBlock(in_channels=1920, out_channels=1280), UnetAttentionBlock(n_heads=8, n_embed=160), Upsample(in_channels=1280)), # Bx2560x16x16 -> Bx1280x32x32\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=1920, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx1920x32x32 -> Bx640x32x32\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=1280, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80)), # Bx1280x32x32 -> Bx640x32x32\n",
    "            \n",
    "            ApplyLayer(UnetResidualBlock(in_channels=960, out_channels=640), UnetAttentionBlock(n_heads=8, n_embed=80), Upsample(in_channels=640)), # Bx960x32x32 -> Bx640x64x64\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=960, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx960x32x32 -> Bx320x64x64\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx640x32x32 -> Bx320x64x64\n",
    "            ApplyLayer(UnetResidualBlock(in_channels=640, out_channels=320), UnetAttentionBlock(n_heads=8, n_embed=40)), # Bx640x32x32 -> Bx320x64x64  \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of the Diffusion Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_embeddings = TimeEmbeddings(time_embedding_dim=320)\n",
    "        self.unet = UNet()\n",
    "        self.head = Head(in_channels=320, out_channels=4)\n",
    "        \n",
    "    def forward(self, latents: torch.Tensor, time: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        # latents: (batch_size, latent_dim, height, width) Bx4x64x64\n",
    "        # time: (1, 320)\n",
    "        # context: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        time_embed = self.time_embeddings(time) # (1, 320) -> (1, 1280)\n",
    "        unet_out = self.unet(latents, time_embed, context) # (B, 4, 64, 64), (B, 320, 64, 64)\n",
    "        return self.head(unet_out) # (B, 320, 64, 64) -> (B, 4, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 4, 64, 64]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 320, 64, 64]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 320, 64, 64]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 320, 64, 64]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 320, 32, 32]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 640, 32, 32]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 640, 32, 32]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 640, 16, 16]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1280, 16, 16]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1280, 16, 16]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1280, 8, 8]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1280, 8, 8]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1280, 8, 8]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 2560, 8, 8]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 2560, 8, 8]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 2560, 8, 8]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 2560, 16, 16]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 2560, 16, 16]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1920, 16, 16]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1920, 32, 32]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 1280, 32, 32]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 960, 32, 32]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 960, 64, 64]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 640, 64, 64]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n",
      "x: torch.Size([1, 640, 64, 64]), time: torch.Size([1, 1280]), context: torch.Size([1, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 4, 64, 64).to('mps')\n",
    "t = torch.randn(1, 320).to('mps')\n",
    "context = torch.randn(1, 77, 768).to('mps')\n",
    "model = DiffusionModel().to('mps')\n",
    "\n",
    "pred = model(x, t, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------> X SHAPE: torch.Size([1, 4, 64, 64])\n",
      "--------------> PRED SHAPE: torch.Size([1, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'--------------> X SHAPE: {x.shape}')\n",
    "print(f'--------------> PRED SHAPE: {pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
